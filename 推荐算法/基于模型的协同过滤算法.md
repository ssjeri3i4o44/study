# 基于模型的协同过滤算法

基于模型的协同过滤是目前最主流的协同过滤类型：主流方法可以分为：关联算法、聚类算法、分类算法、回归算法、矩阵分解、神经网络、图模型以及隐语义模型。

## 1.用关联算法做协同过滤

## 1.1 Apriror

**频繁项集评估标准：支持度、置信度、提升度**

### 1.1.1 频繁项集

- **支持度**

假设我们有两个想分析关联性的数据X和Y：
$$
support(X,Y)=\frac{number(XY)}{num(AllSamples)}
$$
一般来说，支持度高的数据不一定构成频繁项集，但是支持度太低的数据肯定不构成频繁项集。

- **置性度**

置性度体现了一个数据出现后，另一个数据出现的概率，即数据的条件概率
$$
confidence(X<Y)=P(X|Y)=\frac{P(XY)}{P(Y)}
$$
eg.假设在购物数据中，纸巾对鸡爪的置信度为40%，支持度为1%，则意味着在购物数据中，总共有1%的用户会同时买鸡爪和纸巾；买鸡爪的用户中有40%的用户购买了纸巾

- **提升度**

提升度表示含有Y的条件下，同时含有X的概率，与X总体发生的概率之比：
$$
lift(X<Y)=P(X|Y)P(X)=\frac{confidence(X<Y)}{P(X)}
$$
提升度体现了X和Y之间的相关性,提升度大于1，则X<Y是有效的强关联规则，提升度小于等于1则X<Y是无效的弱关联规则。特殊情况下，如果X和Y读了，则有Lift(X<Y)=1.

### 1.1.2 Apriori

Apriori算法的目标是找到最大的K项频繁项集

Apriori算法采用了迭代的方法，先搜索出候选1项集及对应的支持度，剪枝去掉低于支持度的1项集，得到频繁1项集。然后对剩下的频繁1项集进行连接，得到候选的频繁2项集，筛选去掉低于支持度的候选频繁2项集，得到真正的频繁二项集，以此类推，迭代下去，直到无法找到频繁k+1项集为止，对应的频繁k项集的集合即为算法的输出结果。

![](.\img\Apriori.png)



## 1.2 FP Tree

作为一个挖掘频繁集项的算法，Apriori算法需要多次扫描数据，I/O是很大的瓶颈。FP Tree为了解决这一问题采用了一些技巧，无论多少数据，都只要扫描两次数据集，因此提高了算法效率。

- **FP Tree数据结构**

![](.\img\FP Tree数据结构.png)

- **项头表建立**

![](.\img\建立项头表.png)

- **FP Tree建立**

![](.\img\FP-Tree建立.png)

- **FP Tree算法流程**

　　　　1）扫描数据，得到所有频繁一项集的的计数。然后删除支持度低于阈值的项，将1项频繁集放入项头表，并按照支持度降序排列。

　　　　2）扫描数据，将读到的原始数据剔除非频繁1项集，并按照支持度降序排列。

　　　　3）读入排序后的数据集，插入FP树，插入时按照排序后的顺序，插入FP树中，排序靠前的节点是祖先节点，而靠后的是子孙节点。如果有共用的祖先，则对应的公用祖先节点计数加1。插入后，如果有新节点出现，则项头表对应的节点会通过节点链表链接上新节点。直到所有的数据都插入到FP树后，FP树的建立完成。

　　　　4）从项头表的底部项依次向上找到项头表项对应的条件模式基。从条件模式基递归挖掘得到项头表项项的频繁项集。

　　　　5）如果不限制频繁项集的项数，则返回步骤4所有的频繁项集，否则只返回满足项数要求的频繁项集。

## 1.3 PrefixSpan

**前缀对应的投影数据库**：相同前缀对应的所有后缀的结合

PrefixSpan算法的目标是**`挖掘出满足最小支持度的频繁序列`**。前面的Apriori和FP Tree都是频繁项集挖掘，PrefixSpan是挖掘频繁序列模式的。

**算法流程：**

输入：序列数据集S和支持度阈值α

输出：所有满足支持度要求的频繁序列集

1）找出所有长度为1的前缀和对应的投影数据库

2）对长度为1的前缀进行计数，将支持度低于阈值α的前缀对应的项从数据集S删除，同时得到所有的频繁1项序列，i=1.

3）对于每个长度为i满足支持度要求的前缀进行递归挖掘：

 	a) 找出前缀所对应的投影数据库。如果投影数据库为空，则递归返回。

​	b) 统计对应投影数据库中各项的支持度计数。如果所有项的支持度计数都低于阈值α，则递归回。

​	c) 将满足支持度计数的各个单项和当前的前缀进行合并，得到若干新的前缀。

​	d) 令i=i+1，前缀为合并单项后的各个前缀，分别递归执行第3步。



# 2. 用聚类算法做协同过滤

## 2.1 K-Means

## 2.2 BIRCH

## 2.3 DBSCAN密度聚类算法

## 2.4 谱聚类(spectral clustering)

# 3 分类算法

# 4.回归算法

# 5.矩阵分解

目前主流的矩阵分解推荐算法主要是SVD的一些变种，比如FunkSVD，BiasSVD和SVD++。

**SVD（奇异值分解）**：

- 优点：简化数据，去除噪声点，提高算法的结果；
- 缺点：数据的转换可能难以理解；
- 适用于数据类型：数值型

SVD又称奇异值分解，是线性代数中一种矩阵分解的技术，它能够将任意一个m\*n的矩阵A分解成为U、S、V，U是m\*m的正交矩阵，V是n\**n的正交矩阵，S是m*\*n的矩阵，且A=U*S*V。通过SVD方式将矩阵A分解后，如果只保留前k个最大的奇异值，就实现了对矩阵降维的目的。

缺点：需要不全稀疏矩阵；计算复杂度较高。

**Funk-SVD**：将原始的评分矩阵A分解为P、Q，并采用梯度下降方法优化，使原始评分矩阵中有评分的项分解的均方差最小。

**bisaSVD**:在Funk-SVD上加入用户偏置项(bu)和物品偏置项(bi)。

用户偏置项：用户一些与物品无关想评分因素

物品偏置项：物品与用户无关的偏置项。(一个垃圾山寨货评分不可能高，自带这种烂属性的物品由于这个因素会直接导致用户评分低，与用户无关。)

# 6. 神经网络

目前比较主流的用两层神经网络来做推荐算法的是限制玻尔兹曼机(RBM)

# 7.图模型

用图模型做协同过滤，则将用户之间的相似度放到了一个图模型里面去考虑，常用的算法SimRank系列算法和马尔科夫模型算法。

**SimRank：**被相似对象引用的两个对象也具有相似性。算法思想有点类似于PageRank。

如果我们的二部图是G(V,E)，其中V是节点集合，E是边集合。则某一个子集内两个点的相似度s(a,b)可以用和相关联的另一个子集节点之间相似度表示。
$$
s(a,b)=\frac{C}{|I(a)||I(b)|}\sum_{i=1}|I(a)|\sum_{j=1}|I(b)|s(I_i(a),I_j(b))
$$
其中C为阻尼常数，I(a)，I(b)分别代表a,b相连的二部图另一个子集的节点集合。s(I_i(a), I_i(b))即为相连的二部图另一个子集节点之间的相似度。

其中s(a,a)=1,I(a)另，如果I(b)I(a),I(b)有一个为空，即a，b中某一个点没有相连的另一个子集中的点，此时s(a,b)=0。
$$
s(a,b)=\frac{C}{|I(a)||I(b)|}\sum_{i=1}|I(a)|\sum_{j=1}|I(b)|s(I_i(a),I_j(b))\\

=\frac{C}{|I(a)||I(b)|}\sum_{i=1}N\sum_{j=1}P_{ia}s(a,b)p_{jb}\\=C\sum_{i=1}N\sum_{j=1}N
(\frac{p_{ia}}{\sum_{i=1}Np_{ia}})s(a,b)(\frac{p_{jb}}{\sum_{j=1}Np_{jb}})
$$
其中，p为二部图关联边的权重，N为二部图节点数。

相似矩阵S可以表示为：
$$
S=CW^TSW
$$
其中W是将权重值p构成的矩阵P归一化后的矩阵。

由于节点和自己相似度为1，即矩阵S的对角线上的值都应该改为1：
$$
S=CW^TSW+I-Diag(diag(CW^TSW))
$$
**SimRank++:**SimRank++算法对SimRank主要做了两点改进：

- 考虑边的权值。在SimRank算法中，我们对边的归一化权重是用的比较笼统的关联的边数分之一来度量，没有考虑不同的边可能有不同的权重。而SimRank++在构建转移矩阵W时会考虑不同的边的不同权重。
- 子集节点相似度的证据。在SimRank算法中，我们只要认为有变连接，则为相似。但是没有考虑共同连接的边越多，则意味着两个节点的相似度会越高。SimRank++算法利用共同相连的边数作为证据，在每一轮迭代过程中，对SimRank算法计算出来的节点相似度进行修正（乘以对应的证据值得到当前轮迭代的最终相似度值）

**马尔科夫模型**：基于传导性来找出普通距离度量算法难以找出的相似性。

# 8.隐语义模型

