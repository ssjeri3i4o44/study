# 深度学习算法总结

## 1.深度学习的优势

- 不需要特征工程
- 数据格式简易型：神经网络不需要对数据进行归一化、格式转换等
- 参数数目少量性：只需初始化权重w和偏置b，训练过程中这两个参数会不断修正

## 2.卷积神经网络（CNN）

卷积神经网络时一种**局部连接、权值共享、汇聚**的深层前馈神经网络。

### 2.1.全连接前馈神经网络处理图像时，会存在以下问题：

- 参数过多；
- 局部不变性特征：自然图像中物体都具有局部不变性特征，全连接前馈网络很难提取这些局部特征，一般需要进行数据增强来提高特性；
- 汇聚

### 2.2基本结构：

- **卷积层**（Convolutional Layer）：feature_map尺寸 = [(原始尺寸图片-卷积核尺寸)/步长]+1

卷积核也叫滤波器，包括均值滤波器、高斯滤波器、拉普拉斯滤波器等

特征：局部连接、权值共享

- **池化层**（Pooling Layer）：一般包括MaxPooling和AveragePooling，作用是进行特征选择，降低特征数量，并而减少参数数量

** Zero Padding**(补零)：feature_map尺寸 = （width+2*padding_size-filter_size）/stride+1

- **Flatten层** **& Fully Connected Layer**

  ![](img/Flatten层.png)

### 2.3几种经典的卷积神经网络

- **LeNet-5**：

- **AlexNet**：使用ReLU作为非线性激活函数，Dropout防止过拟合，数据增强提高模型准确率

- **Inception网络**：一个卷积层包含多个不同大小的卷积操作，成为Inception模块。Inception网络由多个Inception模块和少量汇聚层堆叠而成。

  GoolLeNet由9个Inception v1模块和5个汇聚层以及其他卷积层和全连接层构成。

- **ResNet**：通过给非线性的卷积层直连边的方式来提高信息的传播效率。

#### 2.4其他卷积方式

- **转置卷积（反卷积）**：上采样。微步卷积，卷积步长<1
- **空洞卷积（膨胀卷积）**：给卷积核插入“空洞”来变相地增加其大小

## 3.熵

### 3.1信息熵

信息量：假如事件A发生的概率很小，但是这件事发生了，那么我们获取的信息量就很大。

信息熵表示的是对信息量的期望：
$$
H（X）=-\sum_{i=1}^np(x_i)log(p(x_i))
$$

### 3.2 相对熵(KL散度)

如果对同一随机变量x有两个单独的概率分布P(X)和Q(X)，可以采用KL散度(Kullback_Leibler divergence)来衡量两个分布的差异。

在机器学习中，P往往用来表示真实分布，如[1,0,0]表示当前样本属于第一类；Q用来表示模型所预测的分布，如[0.7, 0.6, 0.1]。
$$
D_{KL}(p||q) = \sum_{i=1}^np(x_i)log(\frac{p(x_i)}{q(x_i)})\\
D_{KL}的值越小，表示q和p分布越接近
$$

### 3.3交叉熵

相对熵公式通过变形可以得到：
$$
D_{KL}(p||q) = \sum_{i=1}^np(x_i)log(p(x_i))-\sum_{i=1}^np(x_i)log(q(x_i))\\
=-H(p(x))+[-\sum_{i=1}^np(x_i)log(q(x_i))]D_{KL}(p||q) = \sum_{i=1}^np(x_i)log(p(x_i))-\sum_{i=1}^np(x_i)log(q(x_i))\\
=-H(p(x))+[-\sum_{i=1}^np(x_i)log(q(x_i))]
$$
等式的前一部分正好就是p的熵，后一部分就是交叉熵：
$$
H(p,q) = -\sum_{i=1}^np(x_i)log(q(x_i))
$$
在机器学习训练中，需评估label和predict之间的差距，使用KL散度正好。由于KL散度的前一部分是H(p(x))不变的，因此只需要考虑交叉熵。



## 4.批归一化(Batch Normalization)



