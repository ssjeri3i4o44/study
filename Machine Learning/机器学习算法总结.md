# 1.xgboost

1. GBDT只能使用CART作为基学习器，xgboost可以还可以使用线性回归作为基学习器；

2. GBDT中使用损失函数对F(X)的一阶导数，xgboost则对代价函数进行了二阶泰勒展开，同时也用到了一阶导数；

3. GBDT寻找最佳分割点的衡量标准是最小化均方差，

4. xgboost在目标函数中引入L1、L2正则化，控制模型复杂的；

5. 引入skinkage和列采样防止过拟合；

   skinkage：xgboost在一次迭代完成后，会将叶子节点的权重乘上这个系数，为了削弱每个树的影响，让后面有更大学习空间。 

6. 缺失值处理

7. 支持并行化。xgboost的并行不是树粒度的，而是特征粒度上的。xgboost在训练之前，预先对数据进行排序，保存为block结构，迭代过程反复利用这个结构。在进行节点分裂时，需要计算每个特征的增益，选择增益大的特征做分裂，那么各个特征的增益计算可以并行。

   

# 2.GBDT和XGBOOST

GBDT的基本原理是**boost** 里面的 **boosting tree（提升树），并使用 gradient boost。**

- 与传统的Boosting的区别是，每一次的计算是为了减少上一次的残差(residual)，为了消除残差，可以在残差减少的梯度(Gradient)方向上建立一个新的模型。因此GBDT的每个分类器在上一轮分类器的**残差**基础上进行训练。
- GBDT的基学习器都是CATR树，因为gradient boost 需要按照损失函数的梯度近似的拟合残差，这样拟合的是连续数值，因此只有回归树。

xgboost是在GBDT上改进的：

- 将叶节点等加入了正则化项，防止过拟合
- 二阶泰勒展开
- 支持并行化（特征）

# 3.线性回归和逻辑回归

- 线性回归是一个回归问题，即用一条线去拟合训练数据。

$$
f(x)=w_1x_1+w_2x_2+....+w_nx_n+b
$$

训练目标：根据训练数据学习参数（w1,w2,...wn,b)

定义损失函数(均方差损失函数)：
$$
J(W)=1/2(f(x_i)-y_i)^2
$$
为了最小化目标函数，一般采用最小二乘法、梯度下降法等。

- 逻辑回归：将线性回归的输出作为sigmoid函数的输入，最终的输出就是分类结果

# 4.SVM

线性可分（硬间隔）：存在一个超平面，将正负样本完全分开，最大化间隔

线性近似可分（软间隔），加入松弛变量，hinge函数

非线性可分：核函数映射，将数据从低维映射到高维。
$$
线性核：K（x,z) = x*z
$$

$$
多项式核：K（x,z)=(rx*z+r)^d
$$

$$
高斯核函数（RBF）：K(x,z)=exp(-r||x-z||^2)
$$

$$
sigmoid核：K（x,z)=tanh(rx*z+r)
$$

# 5.KNN和K-Means

| KNN                                               | K-Means                                |
| ------------------------------------------------- | -------------------------------------- |
| 分类算法，监督学习                                | 聚类算法无监督学习                     |
| 没有明显的前期训练过程，属于memory-based learning | 有明显的前期训练过程                   |
| K含义：找到数据中离x点最近的K个数据               | K含义：需要人工确定，是将数据分为k个簇 |

# 6.lightgbm

两大改进技术：GOSS和EFB

**GOSS（Gradient-based One-Side Sampling）**：通过区分不同梯度的实例，保留较大梯度实例同时对较小梯度随机采样的方式减少计算量，从而达到提升效率的目的。

**EFB(exclusive feature bundle)**:在稀疏的特征空间中，许多特征是互斥的,（如：动物类别，是否是狗）

1.**速度和内存使用的优化**：LightGBM利用基于直方图优化(Histogram 算法)，将连续的特征分桶（buckets）装进离散的箱子（bins）中：

- 减少分割增益的计算量
- 通过直方图做差进一步加速：叶子的直方图可以通过它的父节点的直方图和兄弟节点的直方图做差得到。利用这种方法，LightGBM 可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。 
- 减少内存使用。
- 减少并行学习的通信代价。

2.**稀疏优化**：对于稀疏的特征仅仅需要O( 2*not_zero_data)来建立直方图

3.**准确率的优化**：

**Leaf-wise的决策树生长策略**：LGB避免了对整层节点分裂法，而采用了对增益最大的节点进行深入分解的方法。但这样容易过拟合，因此可以采用max_depth控制。

4.**类别特征值的最优分割**：

通常将类别特征转化为one-hot coding，但是对于树模型来说，一个基数较大的类别特征，学习树会生长的特别不平衡，并且需要非常深的深度才能来达到较好的准确率。

LightGBM根据训练目标的相关性类别特征重新排序。根据累加值(`sum_gradient / sum_hessian`)重新对（类别特征的）直方图进行排序，然后在排好序的直方图中寻找最好的分割点。

5.**并行学习优化：**

*特征并行*：不再垂直划分数据，即每个线程都有全部数据。

1. 每个线程都在本地数据集上寻找最佳划分点｛特征， 阈值｝
2. 本地进行各个划分的通信整合并得到最佳划分
3. 执行最佳划分。

*数据并行*：使用分散规约 (Reduce scatter) 把直方图合并的任务分摊到不同的机器，降低通信和计算，并利用直方图做差，进一步减少了一半的通信量。

*投票并行*：通过两阶段的投票过程较少特征直方图的通讯开销。

6.**Gradient-based One-Side Sampling**：GOSS是通过区分不同梯度的实例，保留较大梯度实例同时对较小梯度随机采样的方式减少计算量，从而达到提升效率的目的。

#### 类别特征切分流程

- 离散特征建立直方图：统计该特征下每一种离散值出现的次数，并过滤掉出现次数较少的特征值。然后为每一个特征值建立一个bin容器。对于在bin容器内出现次数较少的特征值直接过滤掉，不建立bin容器；

- 先看该特征下划分出的bin容器的个数，如果bin容器的数量小于4，直接使用one vs other方式, 逐个扫描每一个bin容器，找出最佳分裂点；

- bin容器较多的情况, 先进行过滤，只让子集合较大的bin容器参加划分阈值计算, 对每一个符合条件的bin容器进行公式计算:
  $$
  该bin所有样本的一阶梯度之和/该bin所有样本的二阶梯度之和+正则项
  $$
  

# 7.catboost

1.使用对称树（完全二叉树）

2.采用特殊方式处理类别特征

- 计算某特征出现的频率，加上超参数生成新的numerical feartures（要求同一标签的数据不能排列在一起）
- 使用数据的不同排列
- 使用类别特征的不同组合

# 8.最小二乘法

最小二乘的思想就是要使得**观测点和估计点的距离的平方和达到最小**。

# 9.方差和偏差

**偏差**度量了学习算法的**期望预测与真实结果的偏离程度，**即刻画了**算法本身的拟合能力。**

**方差**度量了同样大小的**训练集的变动所导致的学习性能变化，**即刻画了**数据扰动所造成的影响。**

 当模型越复杂时，拟合能力就越好，模型的偏差就越好。

 对于Bagging算法来说，由于我们并行的训练很多的分类器的目的就是降低这个方差，因为采用了相互独立的基分类器的数量多了，h的值就会靠近。所以对于每个基分类器的目的就是如何降低这个偏差，所以我们会采用深度很深并且不剪枝的决策树。

Boosting来说，每一步我们都会在上一轮的基础上更加的拟合原数据，所以可以保证偏差，所以对于每个基分类器来说，问题就是如何选择方差更小的分类器，即更简单的弱分类器，所以我们选择深度很浅的决策树。



*持续更新中。。。*

