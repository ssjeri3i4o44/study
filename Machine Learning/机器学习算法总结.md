# 1.xgboost

1. **基分类器：**GBDT只能使用CART作为基学习器，xgboost可以还可以使用线性回归作为基学习器；

2. **导数信息：**GBDT中使用损失函数对F(X)的一阶导数，xgboost则对代价函数进行了二阶泰勒展开，同时也用到了一阶导数；

   xgboost使用二阶泰勒展开原因：

   *精准性*：相对于一阶泰勒展开，二阶泰勒展开可以更为精准的逼近真实的损失函数

   *可扩展性*：损失函数支持自定义，只需要新的损失函数二阶可导

3. GBDT寻找最佳分割点的衡量标准是最小化均方差，

4. **正则项：**xgboost在目标函数中引入L1、L2正则化，控制模型复杂的；

5. **skinkage：**引入skinkage和列采样防止过拟合；

   skinkage：xgboost在一次迭代完成后，会将叶子节点的权重乘上这个系数，为了削弱每个树的影响，让后面有更大学习空间。 

6. **缺失值处理**：对树中的每个非叶子节点，XGBoost可以自动学习出它的默认分裂方向。如果某样本该特征缺失，会将其划入默认分支。

7. **支持并行化。**xgboost的并行不是树粒度的，而是特征粒度上的。xgboost在训练之前，预先对数据进行排序，保存为block结构，迭代过程反复利用这个结构。在进行节点分裂时，需要计算每个特征的增益，选择增益大的特征做分裂，那么各个特征的增益计算可以并行。

8. **不平衡数据的处理：**XGBoost自带两种解决不平衡数据的方法：第一种，如果在意AUC，采用AUC来评估模型性能，可以通过scale_pos_weight来平衡正负样本权重；第二种，如果在意概率（预测得分的合理性），不能重新平衡数据（会破坏数据的真实分布），那么应该设置max_delta_step为一个有限数字来帮助收敛(基模型为LR时有效)。

   

# 2.GBDT和XGBOOST

GBDT的基本原理是**boost** 里面的 **boosting tree（提升树），并使用 gradient boost。**

- 与传统的Boosting的区别是，每一次的计算是为了减少上一次的残差(residual)，为了消除残差，可以在残差减少的梯度(Gradient)方向上建立一个新的模型。因此GBDT的每个分类器在上一轮分类器的**残差**基础上进行训练。
- GBDT的基学习器都是CATR树，因为gradient boost 需要按照损失函数的梯度近似的拟合残差，这样拟合的是连续数值，因此只有回归树。

xgboost是在GBDT上改进的：

- 将叶节点等加入了正则化项，防止过拟合
- 二阶泰勒展开
- 支持并行化（特征）

# 3.线性回归和逻辑回归

- 线性回归是一个回归问题，即用一条线去拟合训练数据。

$$
f(x)=w_1x_1+w_2x_2+....+w_nx_n+b
$$

训练目标：根据训练数据学习参数（w1,w2,...wn,b)

定义损失函数(均方差损失函数)：
$$
J(W)=1/2(f(x_i)-y_i)^2
$$
为了最小化目标函数，一般采用最小二乘法、梯度下降法等。

- 逻辑回归：将线性回归的输出作为sigmoid函数的输入，最终的输出就是分类结果

# 4.SVM

线性可分（硬间隔）：存在一个超平面，将正负样本完全分开，最大化间隔

线性近似可分（软间隔），加入松弛变量，hinge函数

非线性可分：核函数映射，将数据从低维映射到高维。
$$
线性核：K（x,z) = x*z
$$

$$
多项式核：K（x,z)=(rx*z+r)^d
$$

$$
高斯核函数（RBF）：K(x,z)=exp(-r||x-z||^2)
$$

$$
sigmoid核：K（x,z)=tanh(rx*z+r)
$$

# 5.KNN和K-Means

| KNN                                               | K-Means                                |
| ------------------------------------------------- | -------------------------------------- |
| 分类算法，监督学习                                | 聚类算法无监督学习                     |
| 没有明显的前期训练过程，属于memory-based learning | 有明显的前期训练过程                   |
| K含义：找到数据中离x点最近的K个数据               | K含义：需要人工确定，是将数据分为k个簇 |

# 6.lightgbm

两大改进技术：GOSS和EFB

**GOSS（Gradient-based One-Side Sampling）**：通过区分不同梯度的实例，保留较大梯度实例同时对较小梯度随机采样的方式减少计算量，从而达到提升效率的目的。

**EFB(exclusive feature bundle)**:在稀疏的特征空间中，许多特征是互斥的,（如：动物类别，是否是狗）

1.**速度和内存使用的优化**：LightGBM利用基于直方图优化(Histogram 算法)，将连续的特征分桶（buckets）装进离散的箱子（bins）中：

- 减少分割增益的计算量
- 通过直方图做差进一步加速：叶子的直方图可以通过它的父节点的直方图和兄弟节点的直方图做差得到。利用这种方法，LightGBM 可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。 
- 减少内存使用。
- 减少并行学习的通信代价。

2.**稀疏优化**：对于稀疏的特征仅仅需要O( 2*not_zero_data)来建立直方图

3.**准确率的优化**：

**Leaf-wise的决策树生长策略**：LGB避免了对整层节点分裂法，而采用了对增益最大的节点进行深入分解的方法。但这样容易过拟合，因此可以采用max_depth控制。

4.**类别特征值的最优分割**：

通常将类别特征转化为one-hot coding，但是对于树模型来说，一个基数较大的类别特征，学习树会生长的特别不平衡，并且需要非常深的深度才能来达到较好的准确率。

LightGBM根据训练目标的相关性类别特征重新排序。根据累加值(`sum_gradient / sum_hessian`)重新对（类别特征的）直方图进行排序，然后在排好序的直方图中寻找最好的分割点。

5.**并行学习优化：**

*特征并行*：不再垂直划分数据，即每个线程都有全部数据。

1. 每个线程都在本地数据集上寻找最佳划分点｛特征， 阈值｝
2. 本地进行各个划分的通信整合并得到最佳划分
3. 执行最佳划分。

*数据并行*：使用分散规约 (Reduce scatter) 把直方图合并的任务分摊到不同的机器，降低通信和计算，并利用直方图做差，进一步减少了一半的通信量。

*投票并行*：通过两阶段的投票过程较少特征直方图的通讯开销。

6.**Gradient-based One-Side Sampling**：GOSS是通过区分不同梯度的实例，保留较大梯度实例同时对较小梯度随机采样的方式减少计算量，从而达到提升效率的目的。

#### 类别特征切分流程

- 离散特征建立直方图：统计该特征下每一种离散值出现的次数，并过滤掉出现次数较少的特征值。然后为每一个特征值建立一个bin容器。对于在bin容器内出现次数较少的特征值直接过滤掉，不建立bin容器；

- 先看该特征下划分出的bin容器的个数，如果bin容器的数量小于4，直接使用one vs other方式, 逐个扫描每一个bin容器，找出最佳分裂点；

- bin容器较多的情况, 先进行过滤，只让子集合较大的bin容器参加划分阈值计算, 对每一个符合条件的bin容器进行公式计算:
  $$
  该bin所有样本的一阶梯度之和/该bin所有样本的二阶梯度之和+正则项
  $$
  

# 7.catboost

[参考](https://cloud.tencent.com/developer/news/372336)

**1，类别型特征的处理**

传统是集成学习算法对于类别特征处理都是直接转换为数值型特征，如one-hot编码，但是类别特征并没有大小之分。另一种类别特征的处理方式是采用标签计算一些统计量。
$$
\frac{\sum_{j=1}^{p-1}[x_{\sigma_j,k}=x_{\sigma_p,k}]Y_{\sigma_j}}{\sum_{j=1}^{p-1}[x_{\sigma_j,k}=x_{\sigma_p,k}]Y_{\sigma_j}}
$$
但是这样做容易过拟合。因此Catboost为了避免过拟合，使用了更有效的策略。CatBoost采用了一种有效的策略，降低过拟合的同时也保证了全部数据集都可用于学习。也就是对数据集进行随机排列，计算相同类别值的样本的平均标签值时，只是将这个样本之前的样本的标签值纳入计算。
$$
\frac{\sum_{j=1}^{p-1}[x_{\sigma_j,k}=x_{\sigma_p,k}]Y_{\sigma_j}+aP}{\sum_{j=1}^{p-1}[x_{\sigma_j,k}=x_{\sigma_p,k}]+a}
$$


即
$$
\frac{countInClass+prior}{totalCount+1}
$$
countInClass是具有当前类别特征值的对象标签为1出现的次数，totalCount是具有与当前值匹配的类别特征值的对象综述

其中，先验值P和参数a>0。添加先验这种可以减少低频类别特征的噪声

**2，特征组合**

假设两个类别特征（如用户ID和音乐类别），如果按照上述方法将类别特征转化为数值将失去一些信息，因此将两种类别特征组合可以解决这一问题，得到更强的特征。但是将类别特征组合，特征维度将以指数型增长，并且没有必要将所有类别特征考虑进去。

为当前树构造新的分割点时，CatBoost会采用**贪婪策略**考虑组合。对于树的第一次分割，不考虑任何组合。对于下一个分割，CatBoost将当前树的所有组合、类别型特征与数据集中的所有类别型特征相结合。组合被动态地转换为数字。CatBoost还通过以下方式生成数值型特征和类别型特征的组合：树选择的所有分割点都被视为具有两个值的类别型特征，并且组合方式和类别型特征一样。

**3，克服梯度偏差**

CatBoost，和所有标准梯度提升算法一样，都是通过构建新树来拟合当前模型的梯度。然而，所有经典的提升算法都存在由有偏的点态梯度估计引起的过拟合问题。许多利用GBDT技术的算法（例如，XGBoost、LightGBM），构建一棵树分为两个阶段：

- 选择树结构；
- 在树结构固定后计算叶子节点的值。

为了选择最佳的树结构，算法通过枚举不同的分割，用这些分割构建树，对得到的叶子节点中计算值，然后对得到的树计算评分，最后选择最佳的分割。两个阶段叶子节点的值都是被当做梯度或牛顿步长的近似值来计算。CatBoost第一阶段采用**梯度步长的无偏估计**，第二阶段使用传统的GBDT方案执行。

无偏估计：认为所有样本出现的概率一样大。（因为现实生活中我们不知道某种样本出现的概率）

**4，快速评分**

CatBoost使用oblivious树作为基本预测器，这种树是平衡的，不太容易过拟合。oblivious树中，每个叶子节点的索引可以被编码为长度等于树深度的二进制向量。CatBoost首先将所有浮点特征、统计信息和独热编码特征进行二值化，然后使用二进制特征来计算模型预测值。

# 8.最小二乘法

最小二乘的思想就是要使得**观测点和估计点的距离的平方和达到最小**。

# 9.方差和偏差

**偏差**度量了学习算法的**期望预测与真实结果的偏离程度，**即刻画了**算法本身的拟合能力。**

**方差**度量了同样大小的**训练集的变动所导致的学习性能变化，**即刻画了**数据扰动所造成的影响。**

 当模型越复杂时，拟合能力就越好，模型的偏差就越好。

 对于Bagging算法来说，由于我们并行的训练很多的分类器的目的就是降低这个方差，因为采用了相互独立的基分类器的数量多了，h的值就会靠近。所以对于每个基分类器的目的就是如何降低这个偏差，所以我们会采用深度很深并且不剪枝的决策树。

Boosting来说，每一步我们都会在上一轮的基础上更加的拟合原数据，所以可以保证偏差，所以对于每个基分类器来说，问题就是如何选择方差更小的分类器，即更简单的弱分类器，所以我们选择深度很浅的决策树。

# 10 极大似然估计与贝叶斯估计

[]: https://www.jianshu.com/p/ead99acd6437



- **全概率公式**：如果事件B1,B2,B3...Bn构成一个完备事件，即他们两两互不相容，其和为全集，并且P(Bi)大于0，则对任意事件A有：

$$
P(A)=\frac{P(A|B_1)}{P(B_1)}+\frac{P(A|B_2)}{P(B_2)}+...+\frac{P(A|B_n)}{P(B_n)}
$$

- **先验概率**：是根据以往经验和分析得到的概率。

- **后验概率**：事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性大小

- **极大似然估计**：使得观测数据（样本）发生概率最大的参数就是最好的参数。



- **贝叶斯估计**：

  朴素贝叶斯采用极大似然估计来估计相应的概率，但是这种方法可能会出现概率值为0的情况。贝叶斯估计引入了先验概率，通过先验概率与似然概率来求解后验概率。而最大似然估计是直接通过最大化似然概率来求解得出的。条件概率的贝叶斯估计：

$$
P_{\lambda}(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=2}^NI(x_i^{(j)}=a_{ij},y_i
=c_k)+\lambda}{\sum_{i=1}^NI(y_i=c_k)+S_j\lambda}
$$

等价于在随机变量各个取值的频数上赋予一个正整数lambda。当lambda为0时就是极大似然估计，lambda为1时，是拉普拉斯平滑。

同样，先验概率的贝叶斯估计就是：
$$
P_{\lambda}(Y=c_k)=\frac{\sum_{i=1}^NI(Y_i=c_k)+\lambda}{N+K\lambda}
$$
两者区别：

参数不同：**最大似然估计要估计的参数θ被当作是固定形式的一个未知变量**，然后我们结合真实数据通过最大化似然函数来求解这个固定形式的未知变量。

贝叶斯估计则是将参数视为是有某种已知先验分布的随机变量，意思便是这个参数他不是一个固定的未知数，而是符合一定先验分布如：**随机变量θ符合正态分布等**！那么在贝叶斯估计中除了类条件概率密度p(x|w)符合一定的先验分布，参数θ也符合一定的先验分布。我们通过贝叶斯规则将参数的先验分布转化成后验分布进行求解！

在贝叶斯模型使用过程中，贝叶斯用的是后验概率，而最大似然估计直接使用的是类条件概率密度。

# 11.决策树

决策树就是将一个数据集按照某一个特征分割成子集，使得各个子集在当前条件下能被最好的分类。

优点：简单，易理解，泛化能力好

缺点：容易过拟合（剪枝）

特征选择：ID3：信息增益；C4.5：信息增益比；CART：基尼指数

**熵**：表示随机变量的不确定性
$$
H(X)=-\sum_{i=1}^{n}p_ilogp_i
$$
**信息增益**：得知特征A的信息对数据集D分类的不确定性减少的程度：
$$
g(D,A) = H(D)-H(D|A)
$$
其中H(D)是熵，H(D|A)是条件熵，熵和条件熵之差称之为互信息。决策树中的信息增益等价于训练数据集中类与特征的互信息。

**信息增益比**：信息增益g(D,A)与训练数据集D关于特征A的值的熵H(D)之比：
$$
g_R(D,A)=\frac{g(D,A)}{H_A(D)}
$$
以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题，使用信息增益比可以解决这一问题。

**分类与回归树CART**：递归构建二叉树，对回归树用平方误差最小化准则，对分类树用基尼指数最小化准则。

**基尼指数**：反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率。基尼指数越小，数据集纯度越高。
$$
Gini(D)=\sum_{i=1}^N\sum_{k^`\not=k}p_kp_k'\\=1-\sum_{k=1}^Np_k^2
$$
