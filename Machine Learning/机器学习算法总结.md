# 1.xgboost

1. **基分类器：**GBDT只能使用CART作为基学习器，xgboost的基学习器可以为gbtree，gbliear和dart；

2. **导数信息：**GBDT中使用损失函数对F(X)的一阶导数，xgboost则对代价函数进行了二阶泰勒展开，同时也用到了一阶导数；

   xgboost使用二阶泰勒展开原因：

   *精准性*：相对于一阶泰勒展开，二阶泰勒展开可以更为精准的逼近真实的损失函数

   *可扩展性*：损失函数支持自定义，只需要新的损失函数二阶可导

3. GBDT寻找最佳分割点的衡量标准是最小化均方差，

4. **正则项：**xgboost在目标函数中引入L1、L2正则化，控制模型复杂的；

5. **skinkage：**引入skinkage和列采样防止过拟合；

   skinkage：xgboost在一次迭代完成后，会将叶子节点的权重乘上这个系数，为了削弱每个树的影响，让后面有更大学习空间。 

6. **缺失值处理**：对树中的每个非叶子节点，XGBoost可以自动学习出它的默认分裂方向。如果某样本该特征缺失，会将其划入默认分支。

7. **支持并行化。**xgboost的并行不是树粒度的，而是特征粒度上的。xgboost在训练之前，预先对数据进行排序，保存为block结构，迭代过程反复利用这个结构。在进行节点分裂时，需要计算每个特征的增益，选择增益大的特征做分裂，那么各个特征的增益计算可以并行。

8. **不平衡数据的处理：**XGBoost自带两种解决不平衡数据的方法：第一种，如果在意AUC，采用AUC来评估模型性能，可以通过scale_pos_weight来平衡正负样本权重；第二种，如果在意概率（预测得分的合理性），不能重新平衡数据（会破坏数据的真实分布），那么应该设置max_delta_step为一个有限数字来帮助收敛(基模型为LR时有效)。

   https://mp.weixin.qq.com/s?__biz=MzI1MzY0MzE4Mg==&mid=2247485159&idx=1&sn=d429aac8370ca5127e1e786995d4e8ec&chksm=e9d01626dea79f30043ab80652c4a859760c1ebc0d602e58e13490bf525ad7608a9610495b3d&scene=21#wechat_redirect
   
   注：dart 就是带dropout的树模型，其他和gbtree一样

# 2.lightgbm

两大改进技术：GOSS和EFB

(1) **单边梯度采样(Gradient-based One-Side Sampling,GOSS)**

 LightGBM借鉴了Adaboost算法思想，在Adaboost中样本的权值是样本重要性程度的衡量指标。具有不同的梯度的样本在计算信息增益时扮演着不同的作用，但是在GBDT算法中没有为样本设置权重。LightGBM提出来GOSS方法，保持梯度大的样本，对于梯度小的样本进行随机采样，以此减少计算量，提高算法效率，并且能提高泛华性能。为了保持数据分布一致，GOSS先根据梯度值对数据进行排序，先选出前a%的大梯度实例数据，后面剩下随机采样选择b%的数据。计算信息增益时将选出的小梯度数据乘以常数$$ \frac{1-a}{b}$$，这样算法就会更加关注训练不足的样本，不会过多改变原数据集。

(2) 互斥特征捆绑(Exclusive Feature Bundling,EFB)

LightGBM提出EFB，通过将互斥特征捆绑减少特征维度，以此提高计算速度。在真实环境下，很多特征都是互斥的(特征不会同时为非零值，像one-hot)，这样将两个特征捆绑起来才不会丢失信息。如果两个特征不完全互斥，采用冲突比率对特征不互斥程度进行衡量，然后遍历每个特征尝试合并特征，是冲突比率最小化。

EFB采用贪心算法合并特征使得bundle个数最小。构建一个以特征为图中的点(V)。以特征之间的总冲突为图中的边（E）。这里说的冲突值应该是特征之间cos夹角值，因为要尽可能保证特征之间非0元素不在同一列上。首先按照度来对每个点（特征）做降序排序（度数越大与其他点的冲突越大），然后将特征合并到冲突数小于K的bundle或者新建另外一个bundle。

如何合并同一个bundle的特征来降低训练时间复杂度关键在于原始特征可以从bundle中区分出来。鉴于直方图算法存储离散值而不是连续值，我们可以将互斥特征放在不同的bin中构建bundle，这可以通过偏移量添加到特征原始值中实现。例如，假设bundle中有两个特征，原始特征A取值在[0,10)，B取值在[0,20)，我们添加偏移量10到B中，因此B取值在[10,30)，通过这种方法，就可以将A、B合并，使用一个取值[0,30]的特征取代AB。

其他：

1.**速度和内存使用的优化**：LightGBM利用基于直方图优化(Histogram 算法)，先把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。遍历数据时，根据离散化后的值作为索引在直方图中积累统计量，当遍历一次数据后，直方图积累了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。

- 减少分割增益的计算量
- 通过直方图做差进一步加速：叶子的直方图可以通过它的父节点的直方图和兄弟节点的直方图做差得到。利用这种方法，LightGBM 可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。 
- 减少内存使用。
- 减少并行学习的通信代价。

2.**稀疏优化**：对于稀疏的特征仅仅需要O( 2*not_zero_data)来建立直方图

3.**准确率的优化**：

**Leaf-wise的决策树生长策略**：LGB避免了对整层节点分裂法，而采用了对增益最大的节点进行深入分解的方法。但这样容易过拟合，因此可以采用max_depth控制。

4.**类别特征值的最优分割**：

通常将类别特征转化为one-hot coding，但是对于树模型来说，一个基数较大的类别特征，学习树会生长的特别不平衡，并且需要非常深的深度才能来达到较好的准确率。

LightGBM根据训练目标的相关性类别特征重新排序。根据累加值(`sum_gradient / sum_hessian`)重新对（类别特征的）直方图进行排序，然后在排好序的直方图中寻找最好的分割点。

5.**并行学习优化：**

*特征并行*：不再垂直划分数据，即每个线程都有全部数据。

1. 每个线程都在本地数据集上寻找最佳划分点｛特征， 阈值｝
2. 本地进行各个划分的通信整合并得到最佳划分
3. 执行最佳划分。

*数据并行*：使用分散规约 (Reduce scatter) 把直方图合并的任务分摊到不同的机器，降低通信和计算，并利用直方图做差，进一步减少了一半的通信量。

*投票并行*：通过两阶段的投票过程较少特征直方图的通讯开销。

6.**Gradient-based One-Side Sampling**：GOSS是通过区分不同梯度的实例，保留较大梯度实例同时对较小梯度随机采样的方式减少计算量，从而达到提升效率的目的。

#### 类别特征切分流程

- 离散特征建立直方图：统计该特征下每一种离散值出现的次数，并过滤掉出现次数较少的特征值。然后为每一个特征值建立一个bin容器。对于在bin容器内出现次数较少的特征值直接过滤掉，不建立bin容器；

- 先看该特征下划分出的bin容器的个数，如果bin容器的数量小于4，直接使用one vs other方式, 逐个扫描每一个bin容器，找出最佳分裂点；

- bin容器较多的情况, 先进行过滤，只让子集合较大的bin容器参加划分阈值计算, 对每一个符合条件的bin容器进行公式计算:
  $$
  该bin所有样本的一阶梯度之和/该bin所有样本的二阶梯度之和+正则项
  $$

# 3.catboost

[参考](https://cloud.tencent.com/developer/news/372336)

优势：

1.泛化性能更好

2：虽然训练速度比其他GBDT算法慢，但是预测速度要快很多13-16倍。

**1，类别型特征的处理**

传统是集成学习算法对于类别特征处理都是直接转换为数值型特征，如one-hot编码，但是类别特征并没有大小之分。另一种类别特征的处理方式是采用标签计算一些统计量。
$$
\frac{\sum_{j=1}^{p-1}[x_{\sigma_j,k}=x_{\sigma_p,k}]Y_{\sigma_j}}{\sum_{j=1}^{p-1}[x_{\sigma_j,k}=x_{\sigma_p,k}]Y_{\sigma_j}}
$$
但是这样做容易过拟合。因此Catboost为了避免过拟合，使用了更有效的策略。CatBoost采用了一种有效的策略，降低过拟合的同时也保证了全部数据集都可用于学习。也就是对数据集进行随机排列，计算相同类别值的样本的平均标签值时，只是将这个样本之前的样本的标签值纳入计算。
$$
\frac{\sum_{j=1}^{p-1}[x_{\sigma_j,k}=x_{\sigma_p,k}]Y_{\sigma_j}+aP}{\sum_{j=1}^{p-1}[x_{\sigma_j,k}=x_{\sigma_p,k}]+a}
$$


即
$$
\frac{countInClass+prior}{totalCount+1}
$$
countInClass是具有当前类别特征值的对象标签为1出现的次数，totalCount是具有与当前值匹配的类别特征值的对象综述

其中，先验值P和参数a>0。添加先验这种可以减少低频类别特征的噪声

**2，特征组合**

假设两个类别特征（如用户ID和音乐类别），如果按照上述方法将类别特征转化为数值将失去一些信息，因此将两种类别特征组合可以解决这一问题，得到更强的特征。但是将类别特征组合，特征维度将以指数型增长，并且没有必要将所有类别特征考虑进去。

为当前树构造新的分割点时，CatBoost会采用**贪婪策略**考虑组合。对于树的第一次分割，不考虑任何组合。对于下一个分割，CatBoost将当前树的所有组合、类别型特征与数据集中的所有类别型特征相结合。组合被动态地转换为数字。CatBoost还通过以下方式生成数值型特征和类别型特征的组合：树选择的所有分割点都被视为具有两个值的类别型特征，并且组合方式和类别型特征一样。

**3，克服梯度偏差**

CatBoost，和所有标准梯度提升算法一样，都是通过构建新树来拟合当前模型的梯度。然而，所有经典的提升算法都存在由有偏的点态梯度估计引起的过拟合问题。许多利用GBDT技术的算法（例如，XGBoost、LightGBM），构建一棵树分为两个阶段：

- 选择树结构；
- 在树结构固定后计算叶子节点的值。

为了选择最佳的树结构，算法通过枚举不同的分割，用这些分割构建树，对得到的叶子节点中计算值，然后对得到的树计算评分，最后选择最佳的分割。两个阶段叶子节点的值都是被当做梯度或牛顿步长的近似值来计算。CatBoost第一阶段采用**梯度步长的无偏估计**，第二阶段使用传统的GBDT方案执行。

无偏估计：认为所有样本出现的概率一样大。（因为现实生活中我们不知道某种样本出现的概率）

**梯度步长的无偏估计：**

令Fi为前i颗棵树的结构模型（已构建好），为了使$gi(X_k,Y_k)$是关于模型$F_i$的无偏估计，需要在训练模型$F_i$时不使用样本。由于需要所有训练样本的无偏估计，照以上做法，将没有数据用来训练模型Fi。

样本集为$ {(X_k,Y_k)}_{k=1^n }^n $ 按随机序列排序，树的棵树为$I$。首先，对于样本$ X_k$，初始化模型$M_k$。其次，对于每一颗树，遍历每一个样本，对前K-1个样本，依次计算Loss的梯度$ g_i $,再次，将前k-1个样本的$ g_i $和$X_j (j=1,...,k-1)$用来构建模型M；最后，对每一个样本$ X_k$，用M来修正初始化$ M_k$，这样就可以得到一个分隔的模型$M_K$（并且这个模型不需要这样样本用梯度估计来更新）。重复上述操作，得到每一个样本X的分隔模型M。由此可见，每个$M_k$都共享相同的树结构。

在CatBoost中，构建样本集的s 个随机序列来增强算法的鲁棒性。用不同的序列来训练不同的模型，这将不会导致过拟合。


**4.快速评分**

CatBoost使用oblivious树作为基本预测器，这种树是平衡的，不太容易过拟合。oblivious树中，每个叶子节点的索引可以被编码为长度等于树深度的二进制向量。CatBoost首先将所有浮点特征、统计信息和独热编码特征进行二值化，然后使用二进制特征来计算模型预测值。

# 4.GBDT和XGBOOST

GBDT的基本原理是**boost** 里面的 **boosting tree（提升树），并使用 gradient boost。**

- 与传统的Boosting的区别是，每一次的计算是为了减少上一次的残差(residual)，为了消除残差，可以在残差减少的梯度(Gradient)方向上建立一个新的模型。因此GBDT的每个分类器在上一轮分类器的**残差**基础上进行训练。
- GBDT的基学习器都是CATR树，因为gradient boost 需要按照损失函数的梯度近似的拟合残差，这样拟合的是连续数值，因此只有回归树。

xgboost是在GBDT上改进的：

- 将叶节点等加入了正则化项，防止过拟合
- 二阶泰勒展开
- 支持并行化（特征）

# 5.线性回归和逻辑回归

- 线性回归是一个回归问题，即用一条线去拟合训练数据。

$$
h_{\theta}(x)=\theta _1x_1+\theta _2x_2+....+\theta _n+b
$$

训练目标：根据训练数据学习参数（w1,w2,...wn,b)

定义损失函数(均方差损失函数)：
$$
J(W)=1/2m\sum_{i=1}^m(h_{\theta}(x^i)-y^i)^2
$$
为了最小化目标函数，一般采用最小二乘法、梯度下降法等。



- lasso回归：在损失函数后面加上一个L1正则化

  $$
  J(W)=1/2m\sum_{i=1}^m(h_{\theta}(x^i)-y^i)^2+\lambda \sum_{i=1}^n |\theta_j|
  $$

- 岭回归：在损失函数后面加上一个L2正则化
  $$
  J(W)=1/2m\sum_{i=1}^m(h_{\theta}(x^i)-y^i)^2+\lambda \sum_{i=1}^n \theta_j^2
  $$
  其中$ \lambda$称为正则化参数，如果$\lambda$过大，会把所有$\theta$均最小化，造成欠拟合，如果$\lambda$过小，会导致对过拟合问题解决不当。

  采用lasso回归可以使很多$\theta$项变为0，这样lasso回归的计算量远远小于岭回归。

- 逻辑回归：将线性回归的输出作为sigmoid函数的输入，最终的输出就是分类结果

# 6.SVM

线性可分（硬间隔）：存在一个超平面，将正负样本完全分开，最大化间隔

线性近似可分（软间隔），加入松弛变量，hinge函数

非线性可分：核函数映射，将数据从低维映射到高维。
$$
线性核：K（x,z) = x*z
$$

$$
多项式核：K（x,z)=(rx*z+r)^d
$$

$$
高斯核函数（RBF）：K(x,z)=exp(-r||x-z||^2)
$$

$$
sigmoid核：K（x,z)=tanh(rx*z+r)
$$

# 7.KNN和K-Means

| KNN                                               | K-Means                                |
| ------------------------------------------------- | -------------------------------------- |
| 分类算法，监督学习                                | 聚类算法无监督学习                     |
| 没有明显的前期训练过程，属于memory-based learning | 有明显的前期训练过程                   |
| K含义：找到数据中离x点最近的K个数据               | K含义：需要人工确定，是将数据分为k个簇 |

# 8.最小二乘法

最小二乘的思想就是要使得**观测点和估计点的距离的平方和达到最小**。

# 9.方差和偏差

**偏差**度量了学习算法的**期望预测与真实结果的偏离程度，**即刻画了**算法本身的拟合能力。**

**方差**度量了同样大小的**训练集的变动所导致的学习性能变化，**即刻画了**数据扰动所造成的影响。**

 当模型越复杂时，拟合能力就越好，模型的偏差就越好。

 对于Bagging算法来说，由于我们并行的训练很多的分类器的目的就是降低这个方差，因为采用了相互独立的基分类器的数量多了，h的值就会靠近。所以对于每个基分类器的目的就是如何降低这个偏差，所以我们会采用深度很深并且不剪枝的决策树。

Boosting来说，每一步我们都会在上一轮的基础上更加的拟合原数据，所以可以保证偏差，所以对于每个基分类器来说，问题就是如何选择方差更小的分类器，即更简单的弱分类器，所以我们选择深度很浅的决策树。

# 10 极大似然估计与贝叶斯估计

[]: https://www.jianshu.com/p/ead99acd6437

- **全概率公式**：如果事件B1,B2,B3...Bn构成一个完备事件，即他们两两互不相容，其和为全集，并且P(Bi)大于0，则对任意事件A有：

$$
P(A)=\frac{P(A|B_1)}{P(B_1)}+\frac{P(A|B_2)}{P(B_2)}+...+\frac{P(A|B_n)}{P(B_n)}
$$

- **先验概率**：是根据以往经验和分析得到的概率。

- **后验概率**：事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性大小

- **极大似然估计**：使得观测数据（样本）发生概率最大的参数就是最好的参数。



- **贝叶斯估计**：

  朴素贝叶斯采用极大似然估计来估计相应的概率，但是这种方法可能会出现概率值为0的情况。贝叶斯估计引入了先验概率，通过先验概率与似然概率来求解后验概率。而最大似然估计是直接通过最大化似然概率来求解得出的。条件概率的贝叶斯估计：

$$
P_{\lambda}(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=2}^NI(x_i^{(j)}=a_{ij},y_i
=c_k)+\lambda}{\sum_{i=1}^NI(y_i=c_k)+S_j\lambda}
$$

等价于在随机变量各个取值的频数上赋予一个正整数lambda。当lambda为0时就是极大似然估计，lambda为1时，是拉普拉斯平滑。

同样，先验概率的贝叶斯估计就是：
$$
P_{\lambda}(Y=c_k)=\frac{\sum_{i=1}^NI(Y_i=c_k)+\lambda}{N+K\lambda}
$$
两者区别：

参数不同：**最大似然估计要估计的参数θ被当作是固定形式的一个未知变量**，然后我们结合真实数据通过最大化似然函数来求解这个固定形式的未知变量。

贝叶斯估计则是将参数视为是有某种已知先验分布的随机变量，意思便是这个参数他不是一个固定的未知数，而是符合一定先验分布如：**随机变量θ符合正态分布等**！那么在贝叶斯估计中除了类条件概率密度p(x|w)符合一定的先验分布，参数θ也符合一定的先验分布。我们通过贝叶斯规则将参数的先验分布转化成后验分布进行求解！

在贝叶斯模型使用过程中，贝叶斯用的是后验概率，而最大似然估计直接使用的是类条件概率密度。

# 11.决策树

决策树就是将一个数据集按照某一个特征分割成子集，使得各个子集在当前条件下能被最好的分类。

优点：简单，易理解，泛化能力好

缺点：容易过拟合（剪枝）

特征选择：ID3：信息增益；C4.5：信息增益比；CART：基尼指数

**熵**：表示随机变量的不确定性：
$$
H(X)=-\sum_{i=1}^{n}p_ilogp_i
$$
$p_i$表示当前集合中第$i$类样本所占比例

**信息增益**：得知特征A的信息对数据集D分类的不确定性减少的程度：
$$
g(D,A) = H(D)-H(D|A)
$$
其中H(D)是熵，H(D|A)是条件熵（在已知随机变量A的条件下随机变量D的不确定性），熵和条件熵之差称之为互信息。决策树中的信息增益等价于训练数据集中类与特征的互信息。

特征A对训练数据集D的信息增益g(D,A):集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，

**信息增益比**：信息增益g(D,A)与训练数据集D关于特征A的值的熵H(D)之比：
$$
g_R(D,A)=\frac{g(D,A)}{H_A(D)}
$$
以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题，使用信息增益比可以解决这一问题。

其中，$H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}log_2\frac{D_i}{|D|}$,n是特征A取值的个数。

**分类与回归树CART**：递归构建二叉树，对回归树用平方误差最小化准则，对分类树用基尼指数最小化准则。

**基尼指数**：反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率。基尼指数越小，数据集纯度越高。
$$
Gini(D)=\sum_{i=1}^N\sum_{k^`\not=k}p_kp_k'\\=1-\sum_{k=1}^Np_k^2
$$

# 12.MIC

**MIC**（Maximal Information Coefficient）最大信息数，用于衡量两个变量X与Y之间的关联程度，线性或非线性。

MIC基本原理利用到互信息概念，互信息可以看成是一个随机变量中包含的关于另一个随机变量的信息量，或者说是一个随机变量由已知另一个随机变量而减少的不肯定性。
$$
I(x;y)=\int p(x,y)log_2\frac{p(x,y)}{p(x)p(y)}dxdy
$$
$ p(x,y) $为变量x和y之间的联合概率，一般情况下联合概率计算相对比较麻烦。

MIC的想法是针对两个变量之间的关系，将其离散在二维空间，并且使用散点图来表示，将当期二维空间在x,y方向分布划分为一定的区间数，然后查看当前的散点咋子各方格中落入的情况，这就是联合概率的计算，解决了在互信息中联合概率难求的问题。
$$
mic(x,y)=\max_{a*b<B}\frac{I(x;y)}{log_2min(a,b)}
$$
其中，a,b是在x,y方向上的划分格子的个数，本质上就是网格分布，B是变量，在原文中作者提出B的大小设置是数量的0.6次方左右。

优点：

![](img\MIC优点对比.png)

# 13.缺失值处理

**缺失值缺失原因：**

1. **完全随机缺失(Missing Completely at Random):**所有观测值的缺失概率相同，变量完全随机丢失， 不依赖于任何不完全变量或完全变量，不影响样本的无偏性。如家庭地址缺失。
2. **随机缺失：Missing at Random:** 缺失值的缺失的概率仅取决于可用信息（数据集中的其他变量）,而不取决于变量本身。如财务数据缺失情况与企业大小有关。
3. **非随机缺失：Missing Not At Random-Depends on Unobserved Predictors:** 缺失值取决于未被记录的信息，而这些信息也预测量缺失的值。如某一特定治疗引起不适，患者更有可能退出研究。在这种情况，如果我们删除掉那些缺失的情况，数据样本将是有偏的（如上面的例子，如果把缺失样本删除，则就把引起不适的患者排除在外了）
4. **Missing Not At Random-Depends on Missing Value Itself:** 缺失情况取决于变量本身。

**填充方式：**

1. **特殊值填充：**

   按0，1，-1，平均值，最大值、最小值，中位数等

2. **热卡填充(hot deck imputation, 就近补齐)**：

   在完整数据中找到一个与它最相似的对象，然后用这个相似对象的值来进行填充。

   缺点：相似标准难以定义，缺点较多

3. **K近邻（K-means clustering）**

   根据欧氏距离或者相关分析来确定距离具有缺失样本数据最近的K个样本，将这K个值加权平均来估计该样本的缺失数据

4. **回归**

   基于完整的数据集，建立回归方程，对于包含空值的对象，将已知属性带入方程来估计未知属性，以此估计值来填充，当变量不是线性相关时会导致有偏差的估计

5. **期望值最大化方法（EM）**

   EM算法是一种在不完全数据情况下计算极大似然估计或者后验分布的迭代算法。

   E步(期望步)：在给定完全数据和前一步迭代所得到的参数估计的情况下计算完全数据对应的对数似然函数的条件期望；

   M步(极大化步)：用极大化对数似然函数以确定参数的值，并用于下步的迭代

   缺点：可能陷入局部最优，收敛速度也不是很快，计算很复杂

6. 多重填补(Multiple Imputation, MI)

   多重插补基本步骤：

   - 对于每个缺失值插补n次，每次插补都会得到一个完整的数据集；
   - 插补完后可得到n个完整数据集
   - 对每个完整数据集进行完全数据分析，最后综合分析结果做出统计推断

优点：能更好的体现缺失数据的不确定性；较好地保持变量之间的关系；能提供估计结果不确定性的大量信息。

缺点：由于多重插补主要是为了最终的参数估计，使得其返回的数据意义不大，也不能很好地展示最终的插补结果。

常用方法：

- 连续性变量一般采用得倾向分法：

  倾向得分法是指首先分配一个特殊处理的条件概率给观测协变量，然后观测缺失值的概率目标变量缺失值产生的倾向得分来表示，并根据倾向得分对观测值进行分组，最后对每组数据应用近贝叶斯自助法插补。

- 离散型变量一般采用回归预测法：

  回归预测法指的是对带有缺失值的任一变量，以无缺失值的变量且与缺失数据相关的变量作为辅助变量，建立适当的回归模型（线性回归、logistic回归等），然后根据得到的模型插补缺失值，重复进行上述过程即可完成对有缺 失的每个变量的多重插补.

- 任意缺失模式下采用马尔科夫链蒙特卡罗法(MCMC)

  基于MCMC的多重插补法步骤如下：

  （1）求出观测数据集中均值μ和协方差矩阵∑估计参数α的后验分布；

  （2）根据当前观测值和参数μ、∑，从$P(Y_{mis}|Y_{obs},\alpha^{n})$迭代产生下一刻的插补值$Y_{mis}^{(n+1)}$，进而通过$P(\alpha|Y_{obs},Y_{mis})$求得此刻的参数α。

  重复上述步骤得到马尔科夫链：$(Y_{mis}^{(1)},\alpha^{(1)}),(Y_{mis}^{(2)},\alpha^{(2)}),...(Y_{mis}^{(n)},\alpha^{(n)}),$，直到该马尔科夫链收敛于$P(Y_{mis},\alpha|Y_{obs})$，此时得到的数据可用于插补缺失数据。

# 14.MCMC插补法

蒙特卡罗法（Monte Carlo）：假设概率分布已知，可以根据重复的随机采样来获得数值结果。

*例：计算一个正方形中某个圆的面积，已知正方形面积，通过在正方形中随机抛掷n个点，可以通过这些点落在圆中的比例乘以正方形的面积来估计圆形的面积*

MCMC方法就是通过观测数据集中均值和协方差，随机生成一系列随机参数。选择一个随机参数，同时模型会继续产生随机值（蒙特卡罗部分），然后根据一些规则来确定什么是好的参数。如果随机生成的参数比上一个参数更好，则将根据好的程度确定一定概率将其添加到参数值链中。

MCMC模型生成的样本集计算出的任何数据是我们对该真实后验分布数据的最佳猜测。

# 15 L1、L2正则化

L2正则化就是在原来的损失函数基础上加上权重参数的平方和。

L1正则化就是在原来的损失函数基础上加上权重参数的绝对值。

L1比L2更具有稀疏性，因为L2 的限定区域是平滑的，与中心点等距；而 L1 的限定区域是包含凸点的，尖锐的。这些凸点更接近 Ein 的最优解位置，而在这些凸点上，很多 wj 为 0。

# 16 SMOTE+TOMEKLINKS

SMOTE算法的主要思想是通过在一些位置相近的少数类样本中生成新样本以达到平衡类别的目的。由于SMOTE算法不是简单的复制少数类样本，因此可以在一定程度上避免过拟合。具体地，对于一个少数类样本$x_i$使用K近邻算法，求出离$x_i$距离最近的k个少数类样本，其中距离为样本之间n维特征空间的欧氏距离。然后从k个近邻点中随机选取一个，生成新样本：$x_{new}=x_i+(\hat{x_i}-x_i)*\sigma$,其中$\hat{x}$为k紧邻点，$\sigma$是0-1之间的随机数。

Tomek Links算法主要是用于对数据的重采样和数据清洗。假设样本点$x_i$和$x_j$属于不同的类别，$d(x_i,d_j)$表示两个样本点之间的距离，如果不存在第三个样本点$x_l$使得$d(x_l,x_i)<d(x_i,x_j)$或者$d(x_l,x_j)<d(x_i,x_j)$成立，称$(x_i,x_j)$为Tomek Link对。如果两个样本点为Tomek Link对，则其中某个样本为噪声或者良好个样本都在类别边上。因此可以将Tomek Link都删除，或者将Tomek Link对中的多数类样本删除。

SMOTE+Tomek Links算法就是将SMOTE上采样方法与Tomek Link下采样方法相结合。首先利用SMOTE方法生成新的少数类样本，得到扩充后的数据集后，再删除Tomek Link对。普通SMOTE方法生成的少数类样本是通过线性差值得到的，在平衡类别分布的同时也扩张了少数类的样本空间，产生的问题是可能原本属于多数类样本的空间被少数类“入侵”（invade），容易造成模型的过拟合。Tomek links对寻找的是那种噪声点或者边界点，可以很好地解决“入侵”的问题。